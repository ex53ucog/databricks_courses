{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cb3ef2-d1fe-42b8-b80c-2c59851ee0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sh\n",
    " rm -r /dbfs/hyperopt_lab\n",
    " mkdir /dbfs/hyperopt_lab\n",
    " wget -O /dbfs/hyperopt_lab/penguins.csv https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39da59be-ae5d-483a-b742-2481bd7d6ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/hyperopt_lab/penguins.csv\")\n",
    "data = data.dropna().select(col(\"Island\").astype(\"string\"),\n",
    "                          col(\"CulmenLength\").astype(\"float\"),\n",
    "                          col(\"CulmenDepth\").astype(\"float\"),\n",
    "                          col(\"FlipperLength\").astype(\"float\"),\n",
    "                          col(\"BodyMass\").astype(\"float\"),\n",
    "                          col(\"Species\").astype(\"int\")\n",
    "                          )\n",
    "display(data.sample(0.2))\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1aa3ba-394a-46bd-b627-378b3b6767b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "import mlflow\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "   \n",
    "def objective(params):\n",
    "    # Train a model using the provided hyperparameter value\n",
    "    catFeature = \"Island\"\n",
    "    numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "    catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "    numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "    numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "    featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "    mlAlgo = DecisionTreeClassifier(labelCol=\"Species\",    \n",
    "                                    featuresCol=\"Features\",\n",
    "                                    maxDepth=params['MaxDepth'], maxBins=params['MaxBins'])\n",
    "    pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, mlAlgo])\n",
    "    model = pipeline.fit(train)\n",
    "       \n",
    "    # Evaluate the model to get the target metric\n",
    "    prediction = model.transform(test)\n",
    "    eval = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = eval.evaluate(prediction)\n",
    "    print(f\"accuracy= {accuracy}, {params['MaxDepth']} {params['MaxBins']}\")   \n",
    "    # Hyperopt tries to minimize the objective function, so you must return the negative accuracy.\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18550a2b-bfbd-4870-9ae8-598d15be2d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "   \n",
    "# Define a search space for two hyperparameters (maxDepth and maxBins)\n",
    "search_space = {\n",
    "    'MaxDepth': hp.randint('MaxDepth', 10),\n",
    "    'MaxBins': hp.choice('MaxBins', [10, 20, 30])\n",
    "}\n",
    "   \n",
    "# Specify an algorithm for the hyperparameter optimization process\n",
    "algo=tpe.suggest\n",
    "   \n",
    "# Call the training function iteratively to find the optimal hyperparameter values\n",
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=6)\n",
    "   \n",
    "print(\"Best param values: \", argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae247672-1618-4b16-a468-2733e99bcba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "   \n",
    "# Create a Trials object to track each run\n",
    "trial_runs = Trials()\n",
    "   \n",
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=3,\n",
    "  trials=trial_runs)\n",
    "   \n",
    "print(\"Best param values: \", argmin)\n",
    "   \n",
    "# Get details from each trial run\n",
    "print (\"trials:\")\n",
    "for trial in trial_runs.trials:\n",
    "    print (\"\\n\", trial)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6771249601585422,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hyperparameter Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
