{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ee4657-a7b7-4223-b851-5d4fc1cb6605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sh\n",
    " rm -r /dbfs/mlflow_lab\n",
    " mkdir /dbfs/mlflow_lab\n",
    " wget -O /dbfs/mlflow_lab/penguins.csv https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f5b239-34e4-4741-9678-0e429572197d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/mlflow_lab/penguins.csv\")\n",
    "data = data.dropna().select(col(\"Island\").astype(\"string\"),\n",
    "                            col(\"CulmenLength\").astype(\"float\"),\n",
    "                            col(\"CulmenDepth\").astype(\"float\"),\n",
    "                            col(\"FlipperLength\").astype(\"float\"),\n",
    "                            col(\"BodyMass\").astype(\"float\"),\n",
    "                            col(\"Species\").astype(\"int\")\n",
    "                          )\n",
    "display(data.sample(0.2))\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7618e5-bc74-478e-bcbb-61acbccf79f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "   \n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    catFeature = \"Island\"\n",
    "    numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "     \n",
    "    # parameters\n",
    "    maxIterations = 5\n",
    "    regularization = 0.5\n",
    "   \n",
    "    # Define the feature engineering and model steps\n",
    "    catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "    numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "    numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "    featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "    algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "   \n",
    "    # Chain the steps as stages in a pipeline\n",
    "    pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "   \n",
    "    # Log training parameter values\n",
    "    print (\"Training Logistic Regression model...\")\n",
    "    mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "    mlflow.log_param('regParam', algo.getRegParam())\n",
    "    model = pipeline.fit(train)\n",
    "      \n",
    "    # Evaluate the model and log metrics\n",
    "    prediction = model.transform(test)\n",
    "    metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "    for metric in metrics:\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "        metricValue = evaluator.evaluate(prediction)\n",
    "        print(\"%s: %s\" % (metric, metricValue))\n",
    "        mlflow.log_metric(metric, metricValue)\n",
    "   \n",
    "           \n",
    "    # Log the model itself\n",
    "    unique_model_name = \"classifier-\" + str(time.time())\n",
    "    mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "    modelpath = \"/model/%s\" % (unique_model_name)\n",
    "    mlflow.spark.save_model(model, modelpath)\n",
    "       \n",
    "    print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885f9c7d-12d7-468e-80e9-80547346224a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_penguin_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    import time\n",
    "   \n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "   \n",
    "        catFeature = \"Island\"\n",
    "        numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "   \n",
    "        # Define the feature engineering and model steps\n",
    "        catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "        algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "   \n",
    "        # Chain the steps as stages in a pipeline\n",
    "        pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "   \n",
    "        # Log training parameter values\n",
    "        print (\"Training Logistic Regression model...\")\n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = pipeline.fit(training_data)\n",
    "   \n",
    "        # Evaluate the model and log metrics\n",
    "        prediction = model.transform(test_data)\n",
    "        metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metricValue = evaluator.evaluate(prediction)\n",
    "            print(\"%s: %s\" % (metric, metricValue))\n",
    "            mlflow.log_metric(metric, metricValue)\n",
    "   \n",
    "   \n",
    "        # Log the model itself\n",
    "        unique_model_name = \"classifier-\" + str(time.time())\n",
    "        mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "        modelpath = \"/model/%s\" % (unique_model_name)\n",
    "        mlflow.spark.save_model(model, modelpath)\n",
    "   \n",
    "        print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43257273-4bac-4952-8b07-4f1aca965834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_penguin_model(train, test, 10, 0.2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5371628756761886,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
